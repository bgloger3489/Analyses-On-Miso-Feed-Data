{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x1191bb320>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x119391e88>)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "aliases_list = [\n",
    "    [\"PMI\", \"philip morris\"],\n",
    "    [\"FDA\",\"food and drug\", \"fda\",\"food & drug\"],\n",
    "    [\"Andre Calantzopoulos\",\"calantzopoulos\"],\n",
    "    ['IQOS', 'iqos'],\n",
    "    [\"JUUL\",\"juul\"],\n",
    "    [\"WHO\",\"World Health Organisation\"],\n",
    "    [\"the Royal College of Physicians\",\"royal college of physicians\"],\n",
    "    [\"CDC\",\"centers for disease control\"],\n",
    "    [\"ATHRA\", \"australian tobacco harm reduction association\"],\n",
    "    [\"PHE\", \"public health england\", \"phe\"]\n",
    "] # load in alias list to account for synonyms: [[\"real name\",\"alias1\",\"alias2\", ...], ...] accounts for synonyms\n",
    "\n",
    "MIN_COUNT = 3  # MINIMUM NUMBER OF INSTANCES THAT A NAMED ENTITY MUST APPEAR\n",
    "PRO_INDUSTRY_PREDICTION_THRESHOLD = .75 #PRO-INDUSTRY THRESHOLD WITH WHICH WE CONSIDER A DOCUMENT\n",
    "CHANCE_TO_REPLACE_SNIPPET = 0 # set after feed loaded\n",
    "\n",
    "PATH_TO_FEEDS = 'Feeds/*.jsonl'\n",
    "PATH_TO_CSV = \"coreferences.csv\"\n",
    "\n",
    "BACKGROUND_FOREGROUND_ANALYSIS = True # Toggle background / foreground analysis\n",
    "BACKGROUND_FOREGROUND_THRESHOLD = .35 # Threshold for ratio of foreground frequency to background frequency\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\") #natural language processing model\n",
    "nlp.disable_pipes(\"tagger\", \"parser\") #\"parser\" can't be disabled for spacy noun_chuncks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feeds/*.jsonl\n",
      "['Feeds/DEDUPED_Vaping_search_result_2019-07-24_07_48_25.721746.jsonl', 'Feeds/DEDUPED_Heat_Not_Burn_search_result_2019-07-24_07_48_28.067985.jsonl']\n"
     ]
    }
   ],
   "source": [
    "feed_addresses = glob.glob(PATH_TO_FEEDS)\n",
    "print(feed_addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_stances = [\"POSITIVE_sent_probas\",\"POSITIVE_sent_probas\"] #desired stance for every feed - Must be manually input by user\n",
    "assert len(desired_stances) == len(feed_addresses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2037 documents\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "feeds = [] # 2d-list of feeds with articles of each\n",
    "total_articles = 0\n",
    "for i, adress in enumerate(feed_addresses):\n",
    "    feeds.append([])\n",
    "    for line in open(adress):\n",
    "        feeds[i].append(json.loads(line))\n",
    "        total_articles +=1\n",
    "\n",
    "\n",
    "CHANCE_TO_REPLACE_SNIPPET = 3.0 / total_articles  # percent chance for a current snippet to replace a stored one\n",
    "print(\"Loaded\", total_articles, \"documents\")\n",
    "print(len(feeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_ents = {} #{\"named entity\": count} keeps count of each named entity ex: named_ents[\"PMI\"] ->> 233 times\n",
    "coreference_matrix = {} #{\"named entity\":{\"coref1\":count, \"coref2\":count}} keeps count of coreferenced named entities in a sentence ex: coreference_matrix[\"PMI\"][\"FDA\"] ->>> 34 times\n",
    "associated_keywords = {} #{\"named entity\": {\"keyword1\":count, \"keyword2\":count}} keeps count of keywords associated with named entities ex: associated_keywords[\"PMI\"][\"help\"] ->>> 200 times\n",
    "associated_countries = {} #{\"named entity\": {\"country1\":count, \"country2\":count}} keeps count of countries associated witg named entities ex: associated_countries[\"PMI\"][\"USA\"] ->>> 20 times\n",
    "snippets = {} # {\"named entity\": [snippet1, snippet2, snippet3]} stores example sentences of a named entity\n",
    "background_ents = {} #{\"named entity\": count} keeps count of named entity regardless of whether the sentence is pro-industry\n",
    "\n",
    "\n",
    "\"\"\"add_to_dict will update named_ents\"\"\"\n",
    "def add_to_dict(ent_text): \n",
    "    if ent_text in named_ents:\n",
    "        named_ents[ent_text]+=1\n",
    "    else:\n",
    "        named_ents[ent_text] = 1\n",
    "        \n",
    "\"\"\"add_to_matrix will update coreference_matrix\"\"\"\n",
    "def add_to_matrix(ent1_text,ent2_text): \n",
    "    \n",
    "    if ent1_text not in coreference_matrix:\n",
    "        coreference_matrix[ent1_text] = {}\n",
    "            \n",
    "    if ent2_text not in coreference_matrix:\n",
    "        coreference_matrix[ent2_text] = {}\n",
    "    \n",
    "    if ent1_text not in coreference_matrix[ent2_text]:\n",
    "        coreference_matrix[ent2_text][ent1_text]=1\n",
    "        coreference_matrix[ent1_text][ent2_text]=1\n",
    "    else:\n",
    "        coreference_matrix[ent2_text][ent1_text]+=1 \n",
    "        if not ent2_text == ent1_text: #add it only once if label1 == label2\n",
    "            coreference_matrix[ent1_text][ent2_text]+=1\n",
    "    \n",
    "\"\"\"add_to_keywords will update associated_keywords\"\"\"\n",
    "def add_to_keywords(ent_text, keyword):\n",
    "    if not ent_text in associated_keywords:\n",
    "        associated_keywords[ent_text] = {keyword: 1}\n",
    "    elif not keyword in associated_keywords[ent_text]:\n",
    "        associated_keywords[ent_text][keyword] = 1 \n",
    "    else:\n",
    "        associated_keywords[ent_text][keyword] += 1\n",
    "        \n",
    "\"\"\"add_to_countries will update associated_countries\"\"\"\n",
    "def add_to_countries(ent_text, countries):\n",
    "    if not ent_text in associated_countries:\n",
    "        associated_countries[ent_text] = {countries: 1}\n",
    "    elif not countries in associated_countries[ent_text]:\n",
    "        associated_countries[ent_text][countries] = 1 \n",
    "    else:\n",
    "        associated_countries[ent_text][countries] += 1\n",
    "\n",
    "\"\"\"add_to_snippets will update associated_keywords\"\"\"\n",
    "def add_to_snippets(ent_text, snippet):\n",
    "    if not ent_text in snippets:\n",
    "        snippets[ent_text] = [snippet]\n",
    "    elif snippet in snippets[ent_text]:\n",
    "        return\n",
    "    elif len(snippets[ent_text]) < 3:\n",
    "        snippets[ent_text].append(snippet)\n",
    "    else:\n",
    "        if np.random.random() < CHANCE_TO_REPLACE_SNIPPET:\n",
    "            snippets[int(np.random.randint(0,high=2))] = snippet\n",
    "        \n",
    "def add_to_background_ents(ent_text):\n",
    "    if ent_text in background_ents:\n",
    "        background_ents[ent_text]+=1\n",
    "    else:\n",
    "        background_ents[ent_text]=1\n",
    "        \n",
    "\"\"\"check_alias will check if ent_text is alias, if so it will return the proper name\"\"\"\n",
    "def check_alias(ent_text):\n",
    "    for aliases in aliases_list:\n",
    "        for alias in aliases[1:]:\n",
    "            if alias in ent_text.lower():\n",
    "                return aliases[0]\n",
    "    return ent_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate Over Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f3b07a1f07481dacb854782f10e4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=311), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af26c357fb1487dbdc9879776e21090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=97), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#iterate over every feed\n",
    "for feed_number, feed in enumerate(feeds):\n",
    "    # iterate over every article\n",
    "    for article in tqdm.tqdm_notebook(feed[::5]):\n",
    "        \n",
    "        # iterate over every sentence\n",
    "        for sent, prob in zip(article[\"_source\"][\"sents\"], article[\"_source\"][desired_stances[feed_number]]):\n",
    "\n",
    "            # Condition for BACKGROUND_FOREGROUND_ANALYSIS boolean\n",
    "            if BACKGROUND_FOREGROUND_ANALYSIS or prob > PRO_INDUSTRY_PREDICTION_THRESHOLD: \n",
    "                doc = nlp(sent)\n",
    "                ents = list(doc.ents) \n",
    "\n",
    "                # iterate through entities\n",
    "                for i in range(len(ents)):\n",
    "                    if ents[i].label_ == \"PERSON\" or ents[i].label_ == \"ORG\":\n",
    "\n",
    "                        # check alias:\n",
    "                        ent1_text = ents[i].text\n",
    "                        ent1_text = re.sub(\"[\\n,]\", \"\", ent1_text)\n",
    "                        ent1_text = check_alias(ent1_text)\n",
    "\n",
    "                        #check incomplete name:\n",
    "                        if ents[i].label_== \"PERSON\" and not \" \" in ent1_text and ent1_text != \"JUUL\": #JUUL IS MISTAKENLY THOUGHT TO BE A PERSON\n",
    "                            break\n",
    "\n",
    "\n",
    "                        if prob > PRO_INDUSTRY_PREDICTION_THRESHOLD: # if model is > 75% confident that the sentence is pro-industry\n",
    "                            # update named_ents\n",
    "                            add_to_dict(ent1_text)\n",
    "\n",
    "                            # update keywords\n",
    "                            for token in doc:#noun_chunk in doc.noun_chunks:\n",
    "                                if (not token.is_stop) and token.is_alpha and (not token.text.lower() in ent1_text.lower()):\n",
    "                                    add_to_keywords(ent1_text, token.lemma_.lower())\n",
    "                                #add_to_keywords(ent1_text, noun_chunk.text.lower())\n",
    "\n",
    "                            # update countires\n",
    "                            for country in article[\"_source\"][\"countries\"]:\n",
    "                                add_to_countries(ent1_text, country[\"country_name\"])\n",
    "\n",
    "                            # update snippets\n",
    "                            add_to_snippets(ent1_text, re.sub('\"', '\"\"', re.sub(\"\\n\", \"\", doc.text)))\n",
    "\n",
    "                            # iterate corefs\n",
    "                            for j in range(len(ents) - i):\n",
    "                                if ents[j + i].label_ == \"PERSON\" or ents[j + i].label_ == \"ORG\":\n",
    "                                    # check alias:\n",
    "                                    ent2_text = ents[j + i].text\n",
    "                                    ent3_text = re.sub(\"[\\n,]\", \"\", ent2_text)\n",
    "                                    ent2_text = check_alias(ent2_text)\n",
    "\n",
    "                                    # update coreference_matrix\n",
    "                                    add_to_matrix(ent1_text, ent2_text)\n",
    "\n",
    "                        # update background_ents - regardless of pro-industry statement\n",
    "                        if BACKGROUND_FOREGROUND_ANALYSIS:\n",
    "                            add_to_background_ents(ent1_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard any named entities that are below the minimum count threshold from named_ents and associated_keywords\n",
    "# NOTE: only partially deletes from coreference matrix. not accounted for: coref[\"kept\": {\"kept\": 90, \"DISCARD\":1}]\n",
    "# This is handled later\n",
    "\n",
    "# REMOVE ENTITIES BELOW THRESHOLD OR IF THEY HAVE A RELATIVELY HIGH BACKGROUND FREQUENCY:\n",
    "keys = list(named_ents.keys())\n",
    "for i in range(len(named_ents)):\n",
    "    if named_ents[keys[i]] < MIN_COUNT or (BACKGROUND_FOREGROUND_ANALYSIS and named_ents[keys[i]]/background_ents[keys[i]] < BACKGROUND_FOREGROUND_THRESHOLD):\n",
    "        del named_ents[keys[i]]\n",
    "\n",
    "\n",
    "# REMOVE UNWANTED ENTITIES\n",
    "remove_ents = [\"FDA\",\"CDC\",\"Vaping\",\"Vapes\",\"E-Cigarette\",\"Nicotine\",\"WHO\"]\n",
    "\n",
    "for ent in remove_ents:\n",
    "    if ent in named_ents:\n",
    "        del named_ents[ent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_named_ents = sorted(named_ents.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMAT:\n",
    "# entity, count, snippets, associated keywords, coreferences\n",
    "with open(PATH_TO_CSV, \"w+\") as f:\n",
    "    \n",
    "    if BACKGROUND_FOREGROUND_ANALYSIS:\n",
    "        f.write(\"Entity, Foreground Count, Fore/Back Ratio, Foreground Count*Ratio, Snippets, Associated Keywords, Coreferences\\n\")\n",
    "    else:\n",
    "        f.write(\"Entity, Count, Snippets, , , Associated Keywords, Coreferences\\n\")\n",
    "        \n",
    "    for i, row in enumerate(sorted_named_ents):\n",
    "        ent_text = row[0]\n",
    "        #print(ent_text, named_ents[ent_text],background_ents[ent_text], named_ents[ent_text]/background_ents[ent_text])\n",
    "\n",
    "        # entity\n",
    "        f.write(ent_text)\n",
    "        f.write(',')\n",
    "\n",
    "        # foreground count\n",
    "        f.write(str(named_ents[ent_text]))\n",
    "        f.write(',')\n",
    "        \n",
    "        # ratio\n",
    "        if BACKGROUND_FOREGROUND_ANALYSIS:\n",
    "            f.write(str(named_ents[ent_text]/background_ents[ent_text]))\n",
    "            f.write(',')\n",
    "            \n",
    "        # foregroud count * ratio\n",
    "        if BACKGROUND_FOREGROUND_ANALYSIS:\n",
    "            f.write(str(named_ents[ent_text]**2/background_ents[ent_text]))\n",
    "            f.write(',')\n",
    "\n",
    "        # snippets\n",
    "        f.write('\"')\n",
    "        f.write(snippets[ent_text][0])\n",
    "        f.write('\",\"')\n",
    "        f.write(snippets[ent_text][1] if len(snippets[ent_text])>1 else \"\")\n",
    "        f.write('\",\"')\n",
    "        f.write(snippets[ent_text][2] if len(snippets[ent_text])>2 else \"\")\n",
    "        f.write('\",')\n",
    "\n",
    "        # associated keywords\n",
    "        MIN_KEYWORD_COUNT = 2\n",
    "        f.write('\"')\n",
    "        if ent_text in associated_keywords:\n",
    "            sorted_associated_keywords = sorted(associated_keywords[ent_text].items(), key=lambda kv: kv[1], reverse=True)\n",
    "            for row in sorted_associated_keywords:\n",
    "                keyword = row[0]\n",
    "                if associated_keywords[ent_text][keyword] > MIN_KEYWORD_COUNT:\n",
    "                    f.write(keyword)\n",
    "                    f.write(':')\n",
    "                    f.write(str(associated_keywords[ent_text][keyword]))\n",
    "                    f.write(', ')\n",
    "        f.write('\",')\n",
    "\n",
    "        # coreferences\n",
    "        f.write('\"')\n",
    "        sorted_corefs = sorted(coreference_matrix[ent_text].items(), key=lambda kv: kv[1], reverse= True)\n",
    "        for row in sorted_corefs:\n",
    "            coref = row[0]\n",
    "            if coref in named_ents:  # accounting for incomplete deletion in coreference_matrix\n",
    "                f.write(coref)\n",
    "                f.write(':')\n",
    "                f.write(str(coreference_matrix[ent_text][coref]))\n",
    "                f.write(', ')\n",
    "        f.write('\",')\n",
    "\n",
    "        # COUNRIES\n",
    "        f.write('\"')\n",
    "        if ent_text in associated_countries:\n",
    "            sorted_associated_countries = sorted(associated_countries[ent_text].items(), key=lambda kv: kv[1], reverse= True)\n",
    "            for row in sorted_associated_countries:\n",
    "                country = row[0]\n",
    "                f.write(country)\n",
    "                f.write(':')\n",
    "                f.write(str(associated_countries[ent_text][country]))\n",
    "                f.write(', ')\n",
    "        f.write('\"')\n",
    "\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: noun_chuncks\n",
    "#TODO: deduping\n",
    "#TODO: background / forground - only use Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.Graph()\n",
    "for ent, val in named_ents.items():\n",
    "    graph.add_node(ent,weight=val)\n",
    "for ent, corefs in coreference_matrix.items():\n",
    "    for coref, link_weight in corefs.items():\n",
    "        if coref in named_ents and ent in named_ents:\n",
    "            graph.add_edge(ent,coref,weight=link_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(graph,\"/Users/Ben/Desktop/Vital Strategies/networkxgraphs/feedtest3.graphml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
